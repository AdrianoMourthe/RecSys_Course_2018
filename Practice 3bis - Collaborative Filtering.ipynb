{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2019/20\n",
    "\n",
    "### Practice 3 - Content Based recommenders\n",
    "\n",
    "\n",
    "### Load the data you saw last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-08d964034103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m    \u001b[0mURM_tuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrowSplit\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0muserList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratingList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestampList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mURM_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0muserList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import zipfile, os\n",
    "import numpy as np\n",
    "\n",
    "# If file exists, skip the download\n",
    "data_file_path = \"data/Movielens_10M\"\n",
    "data_file_name = data_file_path + \"/movielens_10m.zip\"\n",
    "\n",
    "os.makedirs(data_file_path, exist_ok=True)   \n",
    "if not os.path.exists(data_file_name):\n",
    "    urlretrieve (\"http://files.grouplens.org/datasets/movielens/ml-10m.zip\", data_file_name)\n",
    "    \n",
    "dataFile = zipfile.ZipFile(data_file_name)\n",
    "URM_path = dataFile.extract(\"ml-10M100K/ratings.dat\", path=\"data/Movielens_10M\")\n",
    "URM_file = open(URM_path, 'r')\n",
    "\n",
    "\n",
    "def rowSplit (rowString):\n",
    "    \n",
    "    split = rowString.split(\"::\")\n",
    "    split[3] = split[3].replace(\"\\n\",\"\")\n",
    "    \n",
    "    split[0] = int(split[0])\n",
    "    split[1] = int(split[1])\n",
    "    split[2] = float(split[2])\n",
    "    split[3] = int(split[3])\n",
    "    \n",
    "    result = tuple(split)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "URM_file.seek(0)\n",
    "URM_tuples = []\n",
    "\n",
    "for line in URM_file:\n",
    "   URM_tuples.append(rowSplit (line))\n",
    "\n",
    "userList, itemList, ratingList, timestampList = zip(*URM_tuples)\n",
    "\n",
    "userList = list(userList)\n",
    "itemList = list(itemList)\n",
    "ratingList = list(ratingList)\n",
    "timestampList = list(timestampList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_ID_stats(ID_list, label):\n",
    "    \n",
    "    min_val = min(ID_list)\n",
    "    max_val = max(ID_list)\n",
    "    unique_val = len(set(ID_list))\n",
    "    missing_val = 1 - unique_val/(max_val - min_val)\n",
    "\n",
    "    print(\"{} data, ID: min {}, max {}, unique {}, missig {:.2f} %\".format(label, min_val, max_val, unique_val, missing_val*100))\n",
    "\n",
    "    \n",
    "    \n",
    "list_ID_stats(userList, \"User\")\n",
    "list_ID_stats(itemList, \"Item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sps\n",
    "\n",
    "URM_all = sps.coo_matrix((ratingList, (userList, itemList)))\n",
    "URM_all = URM_all.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For items in particular most have no interactions. Do we have to remove them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YES!\n",
    "#### Cold items have no impact in the evaluation, since they have no interactions\n",
    "#### Moreover, considering how item-item and user-user CF are defined, they are not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_items_mask = np.ediff1d(URM_all.tocsc().indptr) > 0\n",
    "warm_items = np.arange(URM_all.shape[1])[warm_items_mask]\n",
    "\n",
    "URM_all = URM_all[:, warm_items]\n",
    "URM_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The same holds for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_users_mask = np.ediff1d(URM_all.tocsr().indptr) > 0\n",
    "warm_users = np.arange(URM_all.shape[0])[warm_users_mask]\n",
    "\n",
    "URM_all = URM_all[warm_users, :]\n",
    "URM_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be careful! With this operation we lost the original mapping with item and user IDs!\n",
    "### Keep the warm_items and warm_users array, we might need them in future..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can now build the recommender algorithm, but first we need the train/test split and the evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Notebooks_utils.data_splitter import train_test_holdout\n",
    "\n",
    "\n",
    "URM_train, URM_test = train_test_holdout(URM_all, train_perc = 0.8)\n",
    "\n",
    "\n",
    "from Notebooks_utils.evaluation_function import evaluate_algorithm\n",
    "from Base.Similarity.Compute_Similarity_Python import Compute_Similarity_Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemCFKNNRecommender(object):\n",
    "    \n",
    "    def __init__(self, URM):\n",
    "        self.URM = URM\n",
    "        \n",
    "            \n",
    "    def fit(self, topK=50, shrink=100, normalize=True, similarity=\"cosine\"):\n",
    "        \n",
    "        similarity_object = Compute_Similarity_Python(self.URM, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize, \n",
    "                                                  similarity = similarity)\n",
    "        \n",
    "        self.W_sparse = similarity_object.compute_similarity()\n",
    "\n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our CF recommender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = ItemCFKNNRecommender(URM_train)\n",
    "recommender.fit(shrink=0.0, topK=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's talk about speed\n",
    "\n",
    "#### Time to compute recommendations for a fixed group of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n_users_to_test = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for user_id in range(n_users_to_test):\n",
    "    recommender.recommend(user_id, at=5)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Reasonable implementation speed is {:.2f} usr/sec\".format(n_users_to_test/(end_time-start_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to keep the URM in CSR format!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning\n",
    "\n",
    "#### Once we have built our model we can play with its parameters\n",
    "* Number of neighbors\n",
    "* Shrinkage\n",
    "* Similarity type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tick = [10, 50, 100, 200, 500]\n",
    "MAP_per_k = []\n",
    "\n",
    "for topK in x_tick:\n",
    "    \n",
    "    recommender = ItemCFKNNRecommender(URM_train)\n",
    "    recommender.fit(shrink=0.0, topK=topK)\n",
    "    \n",
    "    result_dict = evaluate_algorithm(URM_test, recommender)\n",
    "    MAP_per_k.append(result_dict[\"MAP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(x_tick, MAP_per_k)\n",
    "pyplot.ylabel('MAP')\n",
    "pyplot.xlabel('TopK')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On this dataset the number of neighbors does not have a great impact on MAP. Higher values of TopK might work even better\n",
    "\n",
    "#### Different datasets will behave in different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tick = [0, 10, 50, 100, 200, 500]\n",
    "MAP_per_shrinkage = []\n",
    "\n",
    "for shrink in x_tick:\n",
    "    \n",
    "    recommender = ItemCFKNNRecommender(URM_train)\n",
    "    recommender.fit(shrink=shrink, topK=100)\n",
    "    \n",
    "    result_dict = evaluate_algorithm(URM_test, recommender)\n",
    "    MAP_per_shrinkage.append(result_dict[\"MAP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyplot.plot(x_tick, MAP_per_shrinkage)\n",
    "pyplot.ylabel('MAP')\n",
    "pyplot.xlabel('Shrinkage')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The shrinkage value (i.e. support) have a much stronger impact. Combine a parameter search with the two to ensure maximum recommendation quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMINDER: Be careful, overfitting!\n",
    "\n",
    "#### While a thorough parameter tuning might result in significantly higher MAP on your validation split, it could have only marginally better or even worse MAP on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserCFKNNRecommender(object):\n",
    "    \n",
    "    def __init__(self, URM):\n",
    "        self.URM = URM\n",
    "        \n",
    "            \n",
    "    def fit(self, topK=50, shrink=100, normalize=True, similarity=\"cosine\"):\n",
    "        \n",
    "        similarity_object = Compute_Similarity_Python(self.URM.T, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize, \n",
    "                                                  similarity = similarity)\n",
    "        \n",
    "        self.W_sparse = similarity_object.compute_similarity()\n",
    "\n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        \n",
    "        scores = self.W_sparse[user_id, :].dot(self.URM).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our CF recommender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = UserCFKNNRecommender(URM_train)\n",
    "recommender.fit(shrink=0.0, topK=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userList_unique = list(set(userList_icm))\n",
    "for user_id in userList_unique[0:10]:\n",
    "    print(recommender.recommend(user_id, at=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's talk about speed\n",
    "\n",
    "#### Time to compute recommendations for a fixed group of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n_users_to_test = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for user_id in range(n_users_to_test):\n",
    "    recommender.recommend(user_id, at=5)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Reasonable implementation speed is {:.2f} usr/sec\".format(n_users_to_test/(end_time-start_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning\n",
    "\n",
    "#### Once we have built our model we can play with its parameters\n",
    "* Number of neighbors\n",
    "* Shrinkage\n",
    "* Similarity type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tick = [10, 50, 100, 200, 500]\n",
    "MAP_per_k = []\n",
    "\n",
    "for topK in x_tick:\n",
    "    \n",
    "    recommender = UserCFKNNRecommender(URM_train)\n",
    "    recommender.fit(shrink=0.0, topK=topK)\n",
    "    \n",
    "    result_dict = evaluate_algorithm(URM_test, recommender)\n",
    "    MAP_per_k.append(result_dict[\"MAP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(x_tick, MAP_per_k)\n",
    "pyplot.ylabel('MAP')\n",
    "pyplot.xlabel('TopK')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On this dataset the number of neighbors does not have a great impact on MAP. Higher values of TopK might work even better\n",
    "\n",
    "#### Different datasets will behave in different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tick = [0, 10, 50, 100, 200, 500]\n",
    "MAP_per_shrinkage = []\n",
    "\n",
    "for shrink in x_tick:\n",
    "    \n",
    "    recommender = UserCFKNNRecommender(URM_train, ICM_all)\n",
    "    recommender.fit(shrink=shrink, topK=100)\n",
    "    \n",
    "    result_dict = evaluate_algorithm(URM_test, recommender)\n",
    "    MAP_per_shrinkage.append(result_dict[\"MAP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyplot.plot(x_tick, MAP_per_shrinkage)\n",
    "pyplot.ylabel('MAP')\n",
    "pyplot.xlabel('Shrinkage')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now load the content information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICM_path = dataFile.extract(\"ml-10M100K/tags.dat\", path = \"data/Movielens_10M\")\n",
    "ICM_file = open(ICM_path, 'r')\n",
    "\n",
    "def rowSplit (rowString):\n",
    "    split = rowString.split(\"::\")\n",
    "    split[3] = split[3].replace(\"\\n\",\"\")\n",
    "    \n",
    "    split[0] = int(split[0])\n",
    "    split[1] = int(split[1])\n",
    "    split[2] = str(split[2]) # tag is a string, not a float like the rating\n",
    "    split[3] = int(split[3])\n",
    "    \n",
    "    result = tuple(split)\n",
    "    \n",
    "    return result\n",
    "\n",
    "ICM_file.seek(0)\n",
    "ICM_tuples = []\n",
    "\n",
    "for line in ICM_file:\n",
    "    ICM_tuples.append(rowSplit(line))\n",
    "    \n",
    "userList_icm, itemList_icm, tagList_icm, timestampList_icm = zip(*ICM_tuples)\n",
    "\n",
    "userList_icm = list(userList_icm)\n",
    "itemList_icm = list(itemList_icm)\n",
    "tagList_icm = list(tagList_icm)\n",
    "timestampList_icm = list(timestampList_icm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ID_stats(userList_icm, \"Users ICM\")\n",
    "list_ID_stats(itemList_icm, \"Items ICM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The numbers of items and users in the ICM matrix is different from what we saw in the URM, why?\n",
    "\n",
    "### The tags are string, we should traslate them into numbers so we can use them as indices in the ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(tagList_icm)\n",
    "\n",
    "tagList_icm = le.transform(tagList_icm)\n",
    "\n",
    "print(tagList_icm[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can now build the ICM\n",
    "\n",
    "#### Be careful with the indices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_items = len(warm_items_mask)\n",
    "n_tags = max(tagList_icm) + 1\n",
    "\n",
    "ICM_shape = (n_items, n_tags)\n",
    "\n",
    "ones = np.ones(len(tagList_icm))\n",
    "ICM_all = sps.coo_matrix((ones, (itemList_icm, tagList_icm)), shape = ICM_shape)\n",
    "ICM_all = ICM_all.tocsr()\n",
    "\n",
    "ICM_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember the mapping of the URM? It is not the same of the ICM one anymore!\n",
    "## We have to keep only the warm items also in the ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICM_all = ICM_all[warm_items, :]\n",
    "ICM_all = ICM_all.tocsr()\n",
    "\n",
    "ICM_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also remove the features that have no occurencies anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_features_mask = np.ediff1d(ICM_all.tocsc().indptr) > 0\n",
    "warm_features = np.arange(ICM_all.shape[1])[warm_features_mask]\n",
    "# Don't forget to keep the mapping\n",
    "\n",
    "ICM_all = ICM_all[:, warm_features]\n",
    "ICM_all = ICM_all.tocsr()\n",
    "\n",
    "ICM_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We could have items without features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nofeatures_items_mask = np.ediff1d(ICM_all.tocsr().indptr) <= 0\n",
    "nofeatures_items_mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We might not remove them in some cases, but we will do it for our comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_items_mask_2 = np.ediff1d(ICM_all.tocsr().indptr) > 0\n",
    "warm_items_2 = np.arange(ICM_all.shape[0])[warm_features_mask_2]\n",
    "\n",
    "ICM_all = ICM_all[warm_items_2, :]\n",
    "ICM_all = ICM_all.tocsr()\n",
    "\n",
    "ICM_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have to remove these items also from the URM and the cold users consequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_all = URM_all[:, warm_items_2]\n",
    "URM_all = URM_all.tocsr()\n",
    "\n",
    "warm_users_mask_2 = np.ediff1d(URM_all.tocsr().indptr) > 0\n",
    "warm_users_2 = np.arange(URM_all.shape[0])[warm_users_mask_2]\n",
    "\n",
    "URM_all = URM_all[warm_users_2, :]\n",
    "URM_all = URM_all.tocsr()\n",
    "\n",
    "URM_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the new URM for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Notebooks_utils.data_splitter import train_test_holdout\n",
    "\n",
    "\n",
    "URM_train, URM_test = train_test_holdout(URM_all, train_perc = 0.8)\n",
    "\n",
    "\n",
    "from Notebooks_utils.evaluation_function import evaluate_algorithm\n",
    "from Base.Similarity.Compute_Similarity_Python import Compute_Similarity_Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are finally ready for the comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite the content-based recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemCBFKNNRecommender(object):\n",
    "    \n",
    "    def __init__(self, URM, ICM):\n",
    "        self.URM = URM\n",
    "        self.ICM = ICM\n",
    "        \n",
    "            \n",
    "    def fit(self, topK=50, shrink=100, normalize=True, similarity=\"cosine\"):\n",
    "        \n",
    "        similarity_object = Compute_Similarity_Python(self.ICM.T, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize, \n",
    "                                                  similarity = similarity)\n",
    "        \n",
    "        self.W_sparse = similarity_object.compute_similarity()\n",
    "\n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_recommender = ItemCBFKNNRecommender(URM_train, ICM_all)\n",
    "collaborative_recommender = ItemCFKNNRecommender(URM_train)\n",
    "\n",
    "for topK in [50, 100, 200]:\n",
    "    for shrink in [10, 50, 100]:\n",
    "        content_recommender.fit(shrink=shrink, topK=topK)\n",
    "        collaborative_recommender.fit(shrink=shrink, topK=topK)\n",
    "        \n",
    "        result_dict = evaluate_algorithm(URM_test, content_recommender)\n",
    "        MAP_per_k.append(result_dict[\"MAP\"])\n",
    "        \n",
    "        result_dict = evaluate_algorithm(URM_test, collaborative_recommender)\n",
    "        MAP_per_k.append(result_dict[\"MAP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
