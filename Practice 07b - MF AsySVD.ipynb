{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2022/23\n",
    "\n",
    "### Practice - AsySVD implemented with Python\n",
    "\n",
    "AsymmetricSVD is a model-based matrix factorization algorithm in which the user latent factors are represented as a function of their user profile and of a second item factor matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movielens10M: Verifying data consistency...\n",
      "Movielens10M: Verifying data consistency... Passed!\n",
      "DataReader: current dataset is: Movielens10M\n",
      "\tNumber of items: 10681\n",
      "\tNumber of users: 69878\n",
      "\tNumber of interactions in URM_all: 10000054\n",
      "\tValue range in URM_all: 0.50-5.00\n",
      "\tInteraction density: 1.34E-02\n",
      "\tInteractions per user:\n",
      "\t\t Min: 2.00E+01\n",
      "\t\t Avg: 1.43E+02\n",
      "\t\t Max: 7.36E+03\n",
      "\tInteractions per item:\n",
      "\t\t Min: 0.00E+00\n",
      "\t\t Avg: 9.36E+02\n",
      "\t\t Max: 3.49E+04\n",
      "\tGini Index: 0.57\n",
      "\n",
      "\tICM name: ICM_all, Value range: 1.00 / 69.00, Num features: 10126, feature occurrences: 128384, density 1.19E-03\n",
      "\tICM name: ICM_genres, Value range: 1.00 / 1.00, Num features: 20, feature occurrences: 21564, density 1.01E-01\n",
      "\tICM name: ICM_tags, Value range: 1.00 / 69.00, Num features: 10106, feature occurrences: 106820, density 9.90E-04\n",
      "\tICM name: ICM_year, Value range: 6.00E+00 / 2.01E+03, Num features: 1, feature occurrences: 10681, density 1.00E+00\n",
      "\n",
      "\n",
      "Warning: 80 (0.11 %) of 69878 users have no sampled items\n"
     ]
    }
   ],
   "source": [
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "from Data_manager.Movielens.Movielens10MReader import Movielens10MReader\n",
    "\n",
    "data_reader = Movielens10MReader()\n",
    "data_loaded = data_reader.load_data()\n",
    "\n",
    "URM_all = data_loaded.get_URM_all()\n",
    "\n",
    "URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<69878x10681 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8000043 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we need for AsySVD?\n",
    "\n",
    "* Loss function\n",
    "* User factor and Item factor matrices\n",
    "* Computing prediction\n",
    "* Update rule\n",
    "* Training loop and some patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_items = URM_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The two methods are based on two latent factor matrices $ W, V \\in R^{I \\times E}$ with E the embedding size, and biases\n",
    "\n",
    "#### How to compute the predictions\n",
    "$ \\hat{r}_{ui} = \\sum_{k=0}^{E}\\sum_{j=0}^{I} r_{uj}W_{jk}H_{ki}$\n",
    "\n",
    "\n",
    "#### The loss function we are interested in minimizing is\n",
    "$L = |R - RWH|_F + \\alpha|W|_2 + \\beta|H|_2$\n",
    "\n",
    "#### Gradients\n",
    "\n",
    "$\\frac{\\partial}{\\partial W} L = -2(R - RWH)RH + 2\\alpha W $\n",
    "\n",
    "$\\frac{\\partial}{\\partial H} L = -2(R - RWH)RW + 2\\alpha H $\n",
    "\n",
    "\n",
    "#### The update is going to be (we can remove the coefficients)\n",
    "$ W = W - \\frac{\\partial}{\\partial W}$, or \n",
    "\n",
    "$ W = W + l((R - RWH)RH - \\alpha W)$, with $l$ the learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: We create the dense latent factor matrices\n",
    "### In a MF model you have two matrices, one with a row per user and the other with a column per item. The other dimension, columns for the first one and rows for the second one is called latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factors = 10\n",
    "\n",
    "user_profile_factors = np.random.random((n_items, num_factors))\n",
    "item_factors = np.random.random((n_items, num_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83268015, 0.84024123, 0.99456521, ..., 0.25918597, 0.09471632,\n",
       "        0.86196292],\n",
       "       [0.86063324, 0.01208739, 0.38109799, ..., 0.70819048, 0.88845714,\n",
       "        0.53291314],\n",
       "       [0.94926168, 0.71983227, 0.51506809, ..., 0.48813948, 0.57082261,\n",
       "        0.94407572],\n",
       "       ...,\n",
       "       [0.10407294, 0.03068574, 0.83906296, ..., 0.82817777, 0.22792677,\n",
       "        0.59978549],\n",
       "       [0.08701777, 0.41126252, 0.32324311, ..., 0.85788997, 0.94494768,\n",
       "        0.98576588],\n",
       "       [0.63997934, 0.42204321, 0.95740229, ..., 0.76016072, 0.99073497,\n",
       "        0.06241658]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_profile_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05861773, 0.7651285 , 0.70024038, ..., 0.481705  , 0.81016677,\n",
       "        0.1152574 ],\n",
       "       [0.40387444, 0.24274421, 0.74289589, ..., 0.5830678 , 0.94015999,\n",
       "        0.77827218],\n",
       "       [0.86464517, 0.60128236, 0.20057371, ..., 0.70083542, 0.45258294,\n",
       "        0.02065533],\n",
       "       ...,\n",
       "       [0.80002861, 0.32511835, 0.54157556, ..., 0.04413489, 0.88323551,\n",
       "        0.7875761 ],\n",
       "       [0.80074544, 0.49147636, 0.26592538, ..., 0.93730028, 0.86763265,\n",
       "        0.94898977],\n",
       "       [0.86186482, 0.66649598, 0.48786971, ..., 0.59470717, 0.24553824,\n",
       "        0.25834748]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: We sample an interaction and compute the prediction of the current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6862565"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_train_coo = URM_train.tocoo()\n",
    "\n",
    "sample_index = np.random.randint(URM_train_coo.nnz)\n",
    "sample_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60134, 1011, 3.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = URM_train_coo.row[sample_index]\n",
    "item_id = URM_train_coo.col[sample_index]\n",
    "rating = URM_train_coo.data[sample_index]\n",
    "\n",
    "user_profile = URM_train[user_id]\n",
    "\n",
    "(user_id, item_id, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64.83785302, 62.92767598, 63.17999842, 63.20199527, 63.01659713,\n",
       "       63.31245479, 63.11436768, 61.27201204, 63.23221939, 62.48529393])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The estimated user factors may be divided by the square root of the profile length to improve learning\n",
    "estimated_user_factors = user_profile.dot(user_profile_factors).ravel()/np.sqrt(user_profile.nnz)\n",
    "estimated_user_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343.7178484123175"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_rating = np.dot(estimated_user_factors, item_factors[item_id,:])\n",
    "predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first predicted rating is a random prediction, essentially\n",
    "\n",
    "### Step 3: We compute the prediction error and update the latent factor matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-340.7178484123175"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_error = rating - predicted_rating\n",
    "prediction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The error is positive, so we need to increase the prediction our model computes. Meaning, we have to increase the values latent factor matrices\n",
    "\n",
    "### Which latent factors we modify? All the factors of the item and user we used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original value to avoid messing up the updates\n",
    "H_all = item_factors.copy()\n",
    "W_u = user_profile_factors[user_profile.indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05861773, 0.7651285 , 0.70024038, ..., 0.481705  , 0.81016677,\n",
       "        0.1152574 ],\n",
       "       [0.40387444, 0.24274421, 0.74289589, ..., 0.5830678 , 0.94015999,\n",
       "        0.77827218],\n",
       "       [0.86464517, 0.60128236, 0.20057371, ..., 0.70083542, 0.45258294,\n",
       "        0.02065533],\n",
       "       ...,\n",
       "       [0.80002861, 0.32511835, 0.54157556, ..., 0.04413489, 0.88323551,\n",
       "        0.7875761 ],\n",
       "       [0.80074544, 0.49147636, 0.26592538, ..., 0.93730028, 0.86763265,\n",
       "        0.94898977],\n",
       "       [0.86186482, 0.66649598, 0.48786971, ..., 0.59470717, 0.24553824,\n",
       "        0.25834748]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86063324, 0.01208739, 0.38109799, ..., 0.70819048, 0.88845714,\n",
       "        0.53291314],\n",
       "       [0.67079319, 0.85719395, 0.0350964 , ..., 0.28510831, 0.86279411,\n",
       "        0.59299786],\n",
       "       [0.88044625, 0.66813129, 0.60754717, ..., 0.92434201, 0.08687705,\n",
       "        0.66332509],\n",
       "       ...,\n",
       "       [0.81937852, 0.20340655, 0.6920163 , ..., 0.0222942 , 0.09001285,\n",
       "        0.57903703],\n",
       "       [0.8099745 , 0.4414923 , 0.69282439, ..., 0.81132473, 0.48334043,\n",
       "        0.00612573],\n",
       "       [0.54257684, 0.18361609, 0.11396505, ..., 0.7562293 , 0.91916405,\n",
       "        0.78030075]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the update rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "regularization = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-769046.78113353, -793623.69665649, -796064.23861583, ...,\n",
       "        -791028.89924351, -797009.41242191, -765061.1628353 ],\n",
       "       [-769046.78113163, -793623.69666494, -796064.23861237, ...,\n",
       "        -791028.89923928, -797009.41242166, -765061.1628359 ],\n",
       "       [-769046.78113373, -793623.69666305, -796064.23861809, ...,\n",
       "        -791028.89924567, -797009.4124139 , -765061.16283661],\n",
       "       ...,\n",
       "       [-769046.78113312, -793623.6966584 , -796064.23861894, ...,\n",
       "        -791028.89923665, -797009.41241393, -765061.16283576],\n",
       "       [-769046.78113302, -793623.69666078, -796064.23861895, ...,\n",
       "        -791028.89924454, -797009.41241786, -765061.16283003],\n",
       "       [-769046.78113035, -793623.6966582 , -796064.23861316, ...,\n",
       "        -791028.89924399, -797009.41242222, -765061.16283778]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors_update = prediction_error * user_profile.dot(H_all) - regularization * W_u\n",
    "user_factors_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-293.23311105,   -4.11839155, -129.84689321, ..., -241.29314302,\n",
       "        -302.7132139 , -181.57302735],\n",
       "       [-228.55121174, -292.06128677,  -11.95797951, ...,  -97.14149736,\n",
       "        -293.9693559 , -202.04495825],\n",
       "       [-299.98375349, -227.64425954, -207.00217296, ..., -314.93982163,\n",
       "         -29.60056545, -226.00670004],\n",
       "       ...,\n",
       "       [-279.17688881,  -69.30424544, -235.7823044 , ...,   -7.59603791,\n",
       "         -30.66899336, -197.28825183],\n",
       "       [-275.97277076, -150.42431682, -236.05764197, ..., -276.43281766,\n",
       "        -164.68271683,   -2.08714724],\n",
       "       [-184.86562407,  -62.56128159,  -38.82993362, ..., -257.66082309,\n",
       "        -313.17560579, -265.8623991 ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_factors_update = prediction_error * W_u - regularization * item_factors[user_profile.indices]\n",
    "item_factors_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile_factors[user_profile.indices,:] += learning_rate * user_factors_update \n",
    "item_factors[user_profile.indices,:] += learning_rate * item_factors_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check what the new prediction for the same user-item interaction would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335.8291625846479"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_user_factors = user_profile.dot(user_profile_factors).ravel()/np.sqrt(user_profile.nnz)\n",
    "\n",
    "predicted_rating = np.dot(estimated_user_factors, item_factors[item_id,:])\n",
    "predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The value is higher than before, we are moving in the right direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now? Sample another interaction and repeat... a lot of times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING: Initialization must be done with random non-zero values ... otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is 0.00\n",
      "Prediction error is 3.00\n"
     ]
    }
   ],
   "source": [
    "user_profile_factors = np.zeros((n_items, num_factors))\n",
    "item_factors = np.zeros((n_items, num_factors))\n",
    "\n",
    "user_profile = URM_train[user_id]\n",
    "estimated_user_factors = user_profile.dot(user_profile_factors).ravel()/np.sqrt(user_profile.nnz)\n",
    "predicted_rating = np.dot(estimated_user_factors, item_factors[item_id,:])\n",
    "\n",
    "print(\"Prediction is {:.2f}\".format(predicted_rating))\n",
    "\n",
    "prediction_error = rating - predicted_rating\n",
    "\n",
    "print(\"Prediction error is {:.2f}\".format(prediction_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_all = item_factors.copy()\n",
    "W_u = user_profile_factors[user_profile.indices,:]\n",
    "\n",
    "user_profile_factors[user_profile.indices,:] += learning_rate * (prediction_error * user_profile.dot(H_all) - regularization * W_u)\n",
    "item_factors[user_profile.indices,:] += learning_rate * (prediction_error * W_u - regularization * item_factors[user_profile.indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after the update is 0.00\n",
      "Prediction error is 3.00\n"
     ]
    }
   ],
   "source": [
    "estimated_user_factors = user_profile.dot(user_profile_factors).ravel()/np.sqrt(user_profile.nnz)\n",
    "predicted_rating = np.dot(estimated_user_factors, item_factors[item_id,:])\n",
    "\n",
    "print(\"Prediction after the update is {:.2f}\".format(predicted_rating))\n",
    "print(\"Prediction error is {:.2f}\".format(rating - predicted_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the matrices are multiplied, if we initialize one of them as zero, the updates will always be zero and the model will not be able to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put all together in a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100000 in 34.51 seconds, loss is 1871.05. Samples per second 2897.45\n",
      "Iteration 200000 in 68.44 seconds, loss is 1008.91. Samples per second 2922.15\n",
      "Iteration 300000 in 102.65 seconds, loss is 687.28. Samples per second 2922.64\n",
      "Iteration 400000 in 136.81 seconds, loss is 524.98. Samples per second 2923.71\n",
      "Iteration 500000 in 172.00 seconds, loss is 427.02. Samples per second 2907.05\n",
      "Iteration 600000 in 206.93 seconds, loss is 361.07. Samples per second 2899.48\n",
      "Iteration 700000 in 241.92 seconds, loss is 313.46. Samples per second 2893.46\n",
      "Iteration 800000 in 276.11 seconds, loss is 277.43. Samples per second 2897.43\n",
      "Iteration 900000 in 310.14 seconds, loss is 249.20. Samples per second 2901.90\n",
      "Iteration 1000000 in 344.65 seconds, loss is 226.47. Samples per second 2901.46\n"
     ]
    }
   ],
   "source": [
    "URM_train_coo = URM_train.tocoo()\n",
    "\n",
    "num_factors = 10\n",
    "learning_rate = 1e-9    # Notice the low learning rate\n",
    "regularization = 1e-1   # Notice the high regularization\n",
    "\n",
    "user_profile_factors = np.random.random((n_items, num_factors))\n",
    "item_factors = np.random.random((n_items, num_factors))\n",
    "\n",
    "loss = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for sample_num in range(1000000):\n",
    "    \n",
    "    # Randomly pick sample\n",
    "    sample_index = np.random.randint(URM_train_coo.nnz)\n",
    "\n",
    "    user_id = URM_train_coo.row[sample_index]\n",
    "    item_id = URM_train_coo.col[sample_index]\n",
    "    rating = URM_train_coo.data[sample_index]\n",
    "    \n",
    "    # Compute prediction\n",
    "    user_profile = URM_train[user_id]\n",
    "    \n",
    "    if user_profile.nnz == 0:\n",
    "        continue \n",
    "        \n",
    "    estimated_user_factors = user_profile.dot(user_profile_factors).ravel()/np.sqrt(user_profile.nnz)\n",
    "    predicted_rating = np.dot(estimated_user_factors, item_factors[item_id,:])\n",
    "        \n",
    "    # Compute prediction error, or gradient\n",
    "    prediction_error = rating - predicted_rating\n",
    "    loss += prediction_error**2\n",
    "\n",
    "    if np.isnan(loss):\n",
    "        break \n",
    "        \n",
    "    # Copy original value to avoid messing up the updates\n",
    "    H_all = item_factors.copy()\n",
    "    W_u = user_profile_factors[user_profile.indices,:]\n",
    "    \n",
    "    # Apply the updates\n",
    "    user_factors_update = prediction_error * user_profile.dot(H_all) - regularization * W_u\n",
    "    item_factors_update = prediction_error * W_u - regularization * item_factors[user_profile.indices]\n",
    "    \n",
    "    user_profile_factors[user_profile.indices,:] += learning_rate * user_factors_update \n",
    "    item_factors[user_profile.indices,:] += learning_rate * item_factors_update    \n",
    "    \n",
    "    # Print some stats\n",
    "    if (sample_num +1)% 100000 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        samples_per_second = sample_num/elapsed_time\n",
    "        print(\"Iteration {} in {:.2f} seconds, loss is {:.2f}. Samples per second {:.2f}\".format(sample_num+1, elapsed_time, loss/sample_num, samples_per_second))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we see? The loss generally goes down but may oscillate a bit.\n",
    "### With higher learning rates or lower regularization you may see numerical instability (i.e., the loss suddendly explodes and then becomes nan, at which point some model parameters will also become none and the model is ruined)\n",
    "\n",
    "### How long do we train such a model?\n",
    "\n",
    "* An epoch: a complete loop over all the train data\n",
    "* Usually you train for multiple epochs. Depending on the algorithm and data 10s or 100s of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time with the previous training speed is 27572.30 seconds (459.54 minutes, 7.66 hours)\n"
     ]
    }
   ],
   "source": [
    "estimated_seconds = 8e6 * 10 / samples_per_second\n",
    "print(\"Estimated time with the previous training speed is {:.2f} seconds ({:.2f} minutes, {:.2f} hours)\".format(estimated_seconds, estimated_seconds/60, estimated_seconds/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AsySVD can be very slow. Each sample requires to compute dozens (or hundreds) of dot products. Cython does not help in this case because most of the computational cost is already vectorized by numpy. Tools such as PyTorch may become useful in this case because they allow to better parallelize these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
